{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Publisher Bias in Academic Textbooks\n",
    "## Using Bayesian Ensemble Methods and Large Language Models\n",
    "\n",
    "**Author:** Derek Lankeaux  \n",
    "**Institution:** Rochester Institute of Technology  \n",
    "**Project:** MS Applied Statistics - Capstone  \n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This notebook implements a comprehensive framework for detecting and quantifying publisher bias in academic textbooks through:\n",
    "\n",
    "1. **LLM Ensemble Rating System**: GPT-4, Claude-3, Llama-3\n",
    "2. **Multi-Dimensional Bias Assessment**: 5 theoretically grounded dimensions\n",
    "3. **Exploratory Factor Analysis**: Uncover latent bias structure\n",
    "4. **Bayesian Hierarchical Models**: Quantify publisher-type effects with PyMC\n",
    "5. **Comprehensive Validation**: Inter-rater reliability, convergent validity\n",
    "\n",
    "**Dataset**: 150 textbooks, 4,500 passages  \n",
    "**Publisher Types**: For-Profit (n=75), University Press (n=50), Open-Source (n=25)  \n",
    "**Disciplines**: Biology, Chemistry, Computer Science, Economics, Psychology, History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from factor_analyzer import FactorAnalyzer, calculate_bartlett_sphericity, calculate_kmo\n",
    "\n",
    "# Bayesian modeling\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# LLM APIs\n",
    "import openai\n",
    "import anthropic\n",
    "import requests\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Caching for expensive operations\n",
    "from joblib import Memory\n",
    "memory = Memory('./cachedir', verbose=0)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"PyMC: {pm.__version__}\")\n",
    "print(f\"ArviZ: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProjectConfig:\n",
    "    \"\"\"Configuration for the textbook bias detection project.\"\"\"\n",
    "    \n",
    "    # Data parameters\n",
    "    n_textbooks: int = 150\n",
    "    passages_per_textbook: int = 30\n",
    "    n_passages: int = 4500\n",
    "    \n",
    "    # Publisher distribution\n",
    "    n_forprofit: int = 75\n",
    "    n_university: int = 50\n",
    "    n_opensource: int = 25\n",
    "    \n",
    "    # Disciplines\n",
    "    disciplines: List[str] = field(default_factory=lambda: [\n",
    "        'Biology', 'Chemistry', 'Computer Science', \n",
    "        'Economics', 'Psychology', 'History'\n",
    "    ])\n",
    "    \n",
    "    # Bias dimensions\n",
    "    bias_dimensions: List[str] = field(default_factory=lambda: [\n",
    "        'Perspective Balance',\n",
    "        'Source Authority',\n",
    "        'Commercial Framing',\n",
    "        'Certainty Language',\n",
    "        'Ideological Framing'\n",
    "    ])\n",
    "    \n",
    "    # LLM models\n",
    "    llm_models: List[str] = field(default_factory=lambda: [\n",
    "        'gpt-4',\n",
    "        'claude-3',\n",
    "        'llama-3'\n",
    "    ])\n",
    "    \n",
    "    # Rating scale\n",
    "    rating_scale_min: int = 1\n",
    "    rating_scale_max: int = 7\n",
    "    \n",
    "    # Factor analysis\n",
    "    n_factors_expected: int = 4\n",
    "    rotation_method: str = 'varimax'\n",
    "    \n",
    "    # Bayesian MCMC\n",
    "    mcmc_draws: int = 2000\n",
    "    mcmc_tune: int = 1000\n",
    "    mcmc_chains: int = 4\n",
    "    mcmc_target_accept: float = 0.95\n",
    "    \n",
    "    # File paths\n",
    "    data_dir: Path = field(default_factory=lambda: Path('./data'))\n",
    "    results_dir: Path = field(default_factory=lambda: Path('./results'))\n",
    "    figures_dir: Path = field(default_factory=lambda: Path('./figures'))\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Create directories if they don't exist.\"\"\"\n",
    "        for dir_path in [self.data_dir, self.results_dir, self.figures_dir]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize configuration\n",
    "config = ProjectConfig()\n",
    "print(\"âœ“ Configuration initialized\")\n",
    "print(f\"Expected dataset: {config.n_passages} passages from {config.n_textbooks} textbooks\")\n",
    "print(f\"Bias dimensions: {len(config.bias_dimensions)}\")\n",
    "print(f\"LLM ensemble size: {len(config.llm_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation (Simulated Dataset)\n",
    "\n",
    "**Note**: In the actual study, this would load real textbook passages. For demonstration, we generate a realistic simulated dataset with known publisher effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulated_data(config: ProjectConfig, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate simulated textbook bias data with known effects.\n",
    "    \n",
    "    This creates a realistic dataset matching the study design:\n",
    "    - Publisher-type effects on bias dimensions\n",
    "    - Discipline-specific variation\n",
    "    - Textbook-level random effects\n",
    "    - LLM ensemble ratings with inter-rater reliability\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Publisher type effects (ground truth for validation)\n",
    "    publisher_effects = {\n",
    "        'For-Profit': {\n",
    "            'Commercial Framing': 0.8,  # Higher commercial influence\n",
    "            'Perspective Balance': -0.6,  # Lower diversity\n",
    "            'Source Authority': 0.3,\n",
    "            'Certainty Language': 0.4,\n",
    "            'Ideological Framing': 0.2\n",
    "        },\n",
    "        'University Press': {\n",
    "            'Commercial Framing': 0.0,  # Neutral\n",
    "            'Perspective Balance': 0.0,\n",
    "            'Source Authority': 0.1,\n",
    "            'Certainty Language': 0.0,\n",
    "            'Ideological Framing': -0.1\n",
    "        },\n",
    "        'Open-Source': {\n",
    "            'Commercial Framing': -0.7,  # Lowest commercial\n",
    "            'Perspective Balance': 0.6,  # Highest diversity\n",
    "            'Source Authority': -0.2,\n",
    "            'Certainty Language': -0.3,\n",
    "            'Ideological Framing': -0.1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    data_rows = []\n",
    "    textbook_id = 0\n",
    "    \n",
    "    # Generate textbooks for each publisher type\n",
    "    for publisher_type, count in [\n",
    "        ('For-Profit', config.n_forprofit),\n",
    "        ('University Press', config.n_university),\n",
    "        ('Open-Source', config.n_opensource)\n",
    "    ]:\n",
    "        for _ in range(count):\n",
    "            textbook_id += 1\n",
    "            discipline = np.random.choice(config.disciplines)\n",
    "            \n",
    "            # Textbook-level random effects\n",
    "            textbook_effects = {dim: np.random.normal(0, 0.3) \n",
    "                              for dim in config.bias_dimensions}\n",
    "            \n",
    "            # Generate passages for this textbook\n",
    "            for passage_id in range(config.passages_per_textbook):\n",
    "                passage_data = {\n",
    "                    'textbook_id': textbook_id,\n",
    "                    'passage_id': f\"{textbook_id}_{passage_id}\",\n",
    "                    'publisher_type': publisher_type,\n",
    "                    'discipline': discipline\n",
    "                }\n",
    "                \n",
    "                # Generate LLM ratings for each dimension\n",
    "                for dimension in config.bias_dimensions:\n",
    "                    # True score\n",
    "                    true_score = (\n",
    "                        4.0 +  # Baseline\n",
    "                        publisher_effects[publisher_type][dimension] +\n",
    "                        textbook_effects[dimension] +\n",
    "                        np.random.normal(0, 0.5)  # Passage noise\n",
    "                    )\n",
    "                    \n",
    "                    # LLM ensemble ratings with reliability Î± â‰ˆ 0.84\n",
    "                    for model in config.llm_models:\n",
    "                        # Model-specific bias (small)\n",
    "                        model_bias = np.random.normal(0, 0.15)\n",
    "                        # Measurement error\n",
    "                        error = np.random.normal(0, 0.4)\n",
    "                        \n",
    "                        rating = true_score + model_bias + error\n",
    "                        # Clip to scale\n",
    "                        rating = np.clip(rating, \n",
    "                                       config.rating_scale_min, \n",
    "                                       config.rating_scale_max)\n",
    "                        \n",
    "                        passage_data[f\"{dimension}_{model}\"] = rating\n",
    "                \n",
    "                data_rows.append(passage_data)\n",
    "    \n",
    "    df = pd.DataFrame(data_rows)\n",
    "    \n",
    "    # Add ensemble averages\n",
    "    for dimension in config.bias_dimensions:\n",
    "        model_cols = [f\"{dimension}_{model}\" for model in config.llm_models]\n",
    "        df[f\"{dimension}_mean\"] = df[model_cols].mean(axis=1)\n",
    "        df[f\"{dimension}_std\"] = df[model_cols].std(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "print(\"Generating simulated dataset...\")\n",
    "df = generate_simulated_data(config)\n",
    "print(f\"âœ“ Generated {len(df)} passages from {df['textbook_id'].nunique()} textbooks\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nPublisher distribution:\")\n",
    "print(df['publisher_type'].value_counts())\n",
    "print(f\"\\nDiscipline distribution:\")\n",
    "print(df['discipline'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "print(\"Sample data (first 3 rows):\")\n",
    "display(df.head(3))\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nSummary statistics for ensemble means:\")\n",
    "mean_cols = [f\"{dim}_mean\" for dim in config.bias_dimensions]\n",
    "display(df[mean_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM Ensemble Integration\n",
    "\n",
    "In production, this section would integrate with actual LLM APIs. Here we demonstrate the prompt engineering and API call structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEnsemble:\n",
    "    \"\"\"Ensemble of LLMs for textbook bias rating.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ProjectConfig, api_keys: Optional[Dict[str, str]] = None):\n",
    "        self.config = config\n",
    "        self.api_keys = api_keys or {}\n",
    "        self.rating_prompt_template = self._create_rating_prompt()\n",
    "    \n",
    "    def _create_rating_prompt(self) -> str:\n",
    "        \"\"\"Create standardized prompt for bias rating.\"\"\"\n",
    "        return \"\"\"\n",
    "You are an expert educational content analyst. Rate the following textbook passage \n",
    "on a scale of 1-7 for each bias dimension. Be objective and consistent.\n",
    "\n",
    "**Passage:**\n",
    "{passage}\n",
    "\n",
    "**Rating Dimensions:**\n",
    "\n",
    "1. **Perspective Balance** (1=Single perspective, 7=Multiple perspectives)\n",
    "   How many viewpoints are presented on the topic?\n",
    "\n",
    "2. **Source Authority** (1=No citations, 7=Diverse authoritative sources)\n",
    "   Quality and diversity of cited sources.\n",
    "\n",
    "3. **Commercial Framing** (1=Strong commercial, 7=No commercial influence)\n",
    "   Presence of market/profit framing vs. academic framing.\n",
    "\n",
    "4. **Certainty Language** (1=Absolute certainty, 7=Appropriate hedging)\n",
    "   Use of qualified vs. unqualified claims.\n",
    "\n",
    "5. **Ideological Framing** (1=Strong ideological, 7=Neutral/balanced)\n",
    "   Presence of political or ideological framing.\n",
    "\n",
    "Provide ratings as JSON:\n",
    "{{\n",
    "  \"Perspective Balance\": <rating>,\n",
    "  \"Source Authority\": <rating>,\n",
    "  \"Commercial Framing\": <rating>,\n",
    "  \"Certainty Language\": <rating>,\n",
    "  \"Ideological Framing\": <rating>,\n",
    "  \"reasoning\": \"<brief justification>\"\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    def rate_passage(self, passage: str, model: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Rate a passage using specified LLM model.\n",
    "        \n",
    "        In production, this would make actual API calls.\n",
    "        For demo, returns simulated ratings.\n",
    "        \"\"\"\n",
    "        # Placeholder for actual API integration\n",
    "        # In production:\n",
    "        # if model == 'gpt-4':\n",
    "        #     return self._call_openai_api(passage)\n",
    "        # elif model == 'claude-3':\n",
    "        #     return self._call_anthropic_api(passage)\n",
    "        # elif model == 'llama-3':\n",
    "        #     return self._call_llama_api(passage)\n",
    "        \n",
    "        # Demo: return simulated ratings\n",
    "        return {dim: np.random.uniform(1, 7) for dim in self.config.bias_dimensions}\n",
    "    \n",
    "    def rate_dataset(self, passages: List[str], \n",
    "                    show_progress: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Rate all passages with ensemble.\n",
    "        Uses caching and parallel processing for efficiency.\n",
    "        \"\"\"\n",
    "        ratings_list = []\n",
    "        \n",
    "        iterator = tqdm(passages, desc=\"Rating passages\") if show_progress else passages\n",
    "        \n",
    "        for passage in iterator:\n",
    "            passage_ratings = {'passage': passage}\n",
    "            \n",
    "            for model in self.config.llm_models:\n",
    "                model_ratings = self.rate_passage(passage, model)\n",
    "                for dim, rating in model_ratings.items():\n",
    "                    passage_ratings[f\"{dim}_{model}\"] = rating\n",
    "            \n",
    "            ratings_list.append(passage_ratings)\n",
    "        \n",
    "        return pd.DataFrame(ratings_list)\n",
    "\n",
    "# Initialize ensemble\n",
    "llm_ensemble = LLMEnsemble(config)\n",
    "print(\"âœ“ LLM Ensemble initialized\")\n",
    "print(f\"Models: {', '.join(config.llm_models)}\")\n",
    "print(f\"Dimensions: {len(config.bias_dimensions)}\")\n",
    "print(f\"\\nSample prompt structure:\")\n",
    "print(llm_ensemble.rating_prompt_template[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Factor Analysis\n",
    "\n",
    "Uncover latent structure in the multi-dimensional bias ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for factor analysis\n",
    "# Use ensemble means for each dimension\n",
    "mean_cols = [f\"{dim}_mean\" for dim in config.bias_dimensions]\n",
    "X_factor = df[mean_cols].values\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_factor_scaled = scaler.fit_transform(X_factor)\n",
    "\n",
    "print(f\"Factor analysis input: {X_factor_scaled.shape}\")\n",
    "print(f\"Variables: {len(mean_cols)}\")\n",
    "print(f\"Observations: {len(X_factor_scaled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test adequacy for factor analysis\n",
    "print(\"=== Factor Analysis Adequacy Tests ===\")\n",
    "print()\n",
    "\n",
    "# Bartlett's test of sphericity\n",
    "chi_square, p_value = calculate_bartlett_sphericity(X_factor_scaled)\n",
    "print(f\"Bartlett's Test of Sphericity:\")\n",
    "print(f\"  Ï‡Â² = {chi_square:.2f}\")\n",
    "print(f\"  p-value = {p_value:.4e}\")\n",
    "print(f\"  Result: {'âœ“ Reject Hâ‚€' if p_value < 0.05 else 'âœ— Fail to reject Hâ‚€'}\")\n",
    "print(f\"  Interpretation: {'Data suitable for FA' if p_value < 0.05 else 'Data may not be suitable'}\")\n",
    "print()\n",
    "\n",
    "# Kaiser-Meyer-Olkin measure\n",
    "kmo_all, kmo_model = calculate_kmo(X_factor_scaled)\n",
    "print(f\"Kaiser-Meyer-Olkin (KMO) Measure:\")\n",
    "print(f\"  Overall KMO = {kmo_model:.3f}\")\n",
    "print(f\"  Assessment: \", end=\"\")\n",
    "if kmo_model >= 0.90:\n",
    "    print(\"Marvelous âœ“\")\n",
    "elif kmo_model >= 0.80:\n",
    "    print(\"Meritorious âœ“\")\n",
    "elif kmo_model >= 0.70:\n",
    "    print(\"Middling âœ“\")\n",
    "elif kmo_model >= 0.60:\n",
    "    print(\"Mediocre\")\n",
    "else:\n",
    "    print(\"Unacceptable âœ—\")\n",
    "print()\n",
    "print(f\"Variable-specific KMO:\")\n",
    "for i, dim in enumerate(config.bias_dimensions):\n",
    "    print(f\"  {dim}: {kmo_all[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal number of factors\n",
    "fa = FactorAnalyzer(n_factors=len(config.bias_dimensions), rotation=None)\n",
    "fa.fit(X_factor_scaled)\n",
    "\n",
    "# Get eigenvalues\n",
    "ev, v = fa.get_eigenvalues()\n",
    "\n",
    "# Scree plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Eigenvalues plot\n",
    "ax1.plot(range(1, len(ev)+1), ev, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.axhline(y=1, color='r', linestyle='--', label='Kaiser criterion (eigenvalue=1)')\n",
    "ax1.set_xlabel('Factor Number', fontsize=12)\n",
    "ax1.set_ylabel('Eigenvalue', fontsize=12)\n",
    "ax1.set_title('Scree Plot', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Cumulative variance explained\n",
    "cumvar = np.cumsum(ev) / np.sum(ev) * 100\n",
    "ax2.plot(range(1, len(ev)+1), cumvar, 'go-', linewidth=2, markersize=8)\n",
    "ax2.axhline(y=70, color='r', linestyle='--', label='70% threshold')\n",
    "ax2.set_xlabel('Number of Factors', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Variance Explained (%)', fontsize=12)\n",
    "ax2.set_title('Cumulative Variance', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.figures_dir / 'scree_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEigenvalues and Variance Explained:\")\n",
    "print(f\"{'Factor':<10} {'Eigenvalue':<12} {'Variance %':<12} {'Cumulative %':<12}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(ev)):\n",
    "    var_pct = (ev[i] / np.sum(ev)) * 100\n",
    "    print(f\"{i+1:<10} {ev[i]:<12.3f} {var_pct:<12.2f} {cumvar[i]:<12.2f}\")\n",
    "\n",
    "# Determine optimal factors (Kaiser criterion + scree)\n",
    "n_factors_kaiser = np.sum(ev > 1)\n",
    "print(f\"\\nKaiser criterion suggests: {n_factors_kaiser} factors\")\n",
    "print(f\"Configuration expects: {config.n_factors_expected} factors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit factor analysis with varimax rotation\n",
    "n_factors = config.n_factors_expected\n",
    "fa = FactorAnalyzer(n_factors=n_factors, \n",
    "                    rotation=config.rotation_method, \n",
    "                    method='ml')\n",
    "fa.fit(X_factor_scaled)\n",
    "\n",
    "# Get loadings\n",
    "loadings = pd.DataFrame(\n",
    "    fa.loadings_,\n",
    "    index=config.bias_dimensions,\n",
    "    columns=[f'Factor {i+1}' for i in range(n_factors)]\n",
    ")\n",
    "\n",
    "print(\"=== Factor Loadings (Varimax Rotation) ===\")\n",
    "print()\n",
    "display(loadings.round(3))\n",
    "\n",
    "# Communalities\n",
    "communalities = pd.DataFrame({\n",
    "    'Dimension': config.bias_dimensions,\n",
    "    'Communality': fa.get_communalities()\n",
    "})\n",
    "print(\"\\nCommunalities (variance explained by factors):\")\n",
    "display(communalities.round(3))\n",
    "\n",
    "# Total variance explained\n",
    "variance = fa.get_factor_variance()\n",
    "variance_df = pd.DataFrame(\n",
    "    variance,\n",
    "    index=['SS Loadings', 'Proportion Var', 'Cumulative Var'],\n",
    "    columns=[f'Factor {i+1}' for i in range(n_factors)]\n",
    ")\n",
    "print(\"\\nFactor Variance:\")\n",
    "display(variance_df.round(3))\n",
    "\n",
    "total_var_explained = variance[1].sum() * 100\n",
    "print(f\"\\nâœ“ {n_factors} factors explain {total_var_explained:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize factor loadings\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(loadings, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "            center=0, vmin=-1, vmax=1, \n",
    "            cbar_kws={'label': 'Loading'},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('Factor Loading Matrix (Varimax Rotation)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Latent Factors', fontsize=12)\n",
    "ax.set_ylabel('Observed Dimensions', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.figures_dir / 'factor_loadings.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Interpret factors based on highest loadings\n",
    "print(\"\\n=== Factor Interpretation ===\")\n",
    "print()\n",
    "for i in range(n_factors):\n",
    "    factor_col = f'Factor {i+1}'\n",
    "    top_loadings = loadings[factor_col].abs().nlargest(3)\n",
    "    print(f\"{factor_col}:\")\n",
    "    for dim, loading in top_loadings.items():\n",
    "        actual_loading = loadings.loc[dim, factor_col]\n",
    "        print(f\"  â€¢ {dim}: {actual_loading:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute factor scores for each passage\n",
    "factor_scores = fa.transform(X_factor_scaled)\n",
    "factor_score_df = pd.DataFrame(\n",
    "    factor_scores,\n",
    "    columns=[f'Factor{i+1}_score' for i in range(n_factors)]\n",
    ")\n",
    "\n",
    "# Add to main dataframe\n",
    "df = pd.concat([df, factor_score_df], axis=1)\n",
    "\n",
    "print(f\"âœ“ Factor scores computed for {len(df)} passages\")\n",
    "print(f\"\\nFactor score summary:\")\n",
    "display(factor_score_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inter-Rater Reliability Analysis\n",
    "\n",
    "Assess consistency across the LLM ensemble using Krippendorff's alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def krippendorff_alpha(data: np.ndarray, level_of_measurement: str = 'interval') -> float:\n",
    "    \"\"\"\n",
    "    Calculate Krippendorff's alpha for inter-rater reliability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : np.ndarray\n",
    "        Matrix of ratings (raters Ã— items)\n",
    "    level_of_measurement : str\n",
    "        'nominal', 'ordinal', 'interval', or 'ratio'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    alpha : float\n",
    "        Krippendorff's alpha coefficient\n",
    "    \"\"\"\n",
    "    # Simplified implementation for interval data\n",
    "    # Full implementation would handle missing data and different measurement levels\n",
    "    \n",
    "    n_raters, n_items = data.shape\n",
    "    \n",
    "    # Calculate observed disagreement\n",
    "    D_o = 0\n",
    "    pairs = 0\n",
    "    \n",
    "    for item in range(n_items):\n",
    "        ratings = data[:, item]\n",
    "        n_ratings = len(ratings)\n",
    "        for i in range(n_ratings):\n",
    "            for j in range(i+1, n_ratings):\n",
    "                D_o += (ratings[i] - ratings[j])**2\n",
    "                pairs += 1\n",
    "    \n",
    "    D_o = D_o / pairs if pairs > 0 else 0\n",
    "    \n",
    "    # Calculate expected disagreement\n",
    "    all_ratings = data.flatten()\n",
    "    n_total = len(all_ratings)\n",
    "    D_e = 0\n",
    "    pairs_e = 0\n",
    "    \n",
    "    for i in range(n_total):\n",
    "        for j in range(i+1, n_total):\n",
    "            D_e += (all_ratings[i] - all_ratings[j])**2\n",
    "            pairs_e += 1\n",
    "    \n",
    "    D_e = D_e / pairs_e if pairs_e > 0 else 0\n",
    "    \n",
    "    # Calculate alpha\n",
    "    alpha = 1 - (D_o / D_e) if D_e > 0 else 0\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "# Calculate reliability for each dimension\n",
    "print(\"=== Inter-Rater Reliability (Krippendorff's Alpha) ===\")\n",
    "print()\n",
    "print(f\"{'Dimension':<25} {'Alpha':<10} {'Assessment'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "reliability_results = {}\n",
    "\n",
    "for dimension in config.bias_dimensions:\n",
    "    # Get ratings from all models for this dimension\n",
    "    model_cols = [f\"{dimension}_{model}\" for model in config.llm_models]\n",
    "    ratings_matrix = df[model_cols].values.T  # raters Ã— items\n",
    "    \n",
    "    # Calculate alpha\n",
    "    alpha = krippendorff_alpha(ratings_matrix)\n",
    "    reliability_results[dimension] = alpha\n",
    "    \n",
    "    # Assessment\n",
    "    if alpha >= 0.90:\n",
    "        assessment = \"Excellent âœ“âœ“\"\n",
    "    elif alpha >= 0.80:\n",
    "        assessment = \"Good âœ“\"\n",
    "    elif alpha >= 0.70:\n",
    "        assessment = \"Acceptable\"\n",
    "    elif alpha >= 0.60:\n",
    "        assessment = \"Questionable\"\n",
    "    else:\n",
    "        assessment = \"Poor âœ—\"\n",
    "    \n",
    "    print(f\"{dimension:<25} {alpha:<10.3f} {assessment}\")\n",
    "\n",
    "# Overall reliability\n",
    "overall_alpha = np.mean(list(reliability_results.values()))\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Overall Mean':<25} {overall_alpha:<10.3f}\")\n",
    "print()\n",
    "print(f\"âœ“ LLM ensemble demonstrates {'excellent' if overall_alpha >= 0.80 else 'acceptable'} reliability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayesian Hierarchical Models\n",
    "\n",
    "Estimate publisher-type effects using PyMC with proper hierarchical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Bayesian modeling\n",
    "# Focus on Factor 1 (likely \"Commercial Influence\" based on loadings)\n",
    "target_factor = 'Factor1_score'\n",
    "\n",
    "# Create numeric encodings\n",
    "publisher_map = {'For-Profit': 0, 'University Press': 1, 'Open-Source': 2}\n",
    "discipline_map = {d: i for i, d in enumerate(config.disciplines)}\n",
    "\n",
    "df['publisher_idx'] = df['publisher_type'].map(publisher_map)\n",
    "df['discipline_idx'] = df['discipline'].map(discipline_map)\n",
    "\n",
    "# Extract data\n",
    "y = df[target_factor].values\n",
    "publisher_idx = df['publisher_idx'].values\n",
    "discipline_idx = df['discipline_idx'].values\n",
    "textbook_idx = df['textbook_id'].values - 1  # 0-indexed\n",
    "\n",
    "n_publishers = len(publisher_map)\n",
    "n_disciplines = len(discipline_map)\n",
    "n_textbooks = df['textbook_id'].nunique()\n",
    "\n",
    "print(f\"Modeling: {target_factor}\")\n",
    "print(f\"Observations: {len(y)}\")\n",
    "print(f\"Publishers: {n_publishers}\")\n",
    "print(f\"Disciplines: {n_disciplines}\")\n",
    "print(f\"Textbooks: {n_textbooks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Bayesian hierarchical model\n",
    "print(\"Building Bayesian hierarchical model...\")\n",
    "print()\n",
    "\n",
    "with pm.Model() as hierarchical_model:\n",
    "    # Hyperpriors for group-level variance\n",
    "    sigma_textbook = pm.HalfNormal('sigma_textbook', sigma=0.5)\n",
    "    sigma_discipline = pm.HalfNormal('sigma_discipline', sigma=0.5)\n",
    "    \n",
    "    # Publisher type effects (fixed effects)\n",
    "    # Using sum-to-zero constraint\n",
    "    publisher_raw = pm.Normal('publisher_raw', mu=0, sigma=1, shape=n_publishers-1)\n",
    "    publisher_effect = pm.Deterministic(\n",
    "        'publisher_effect',\n",
    "        pm.math.concatenate([publisher_raw, [-pm.math.sum(publisher_raw)]])\n",
    "    )\n",
    "    \n",
    "    # Discipline random effects\n",
    "    discipline_effect = pm.Normal(\n",
    "        'discipline_effect',\n",
    "        mu=0,\n",
    "        sigma=sigma_discipline,\n",
    "        shape=n_disciplines\n",
    "    )\n",
    "    \n",
    "    # Textbook random effects\n",
    "    textbook_effect = pm.Normal(\n",
    "        'textbook_effect',\n",
    "        mu=0,\n",
    "        sigma=sigma_textbook,\n",
    "        shape=n_textbooks\n",
    "    )\n",
    "    \n",
    "    # Global intercept\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "    \n",
    "    # Expected value\n",
    "    theta = (\n",
    "        mu +\n",
    "        publisher_effect[publisher_idx] +\n",
    "        discipline_effect[discipline_idx] +\n",
    "        textbook_effect[textbook_idx]\n",
    "    )\n",
    "    \n",
    "    # Residual variance\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Normal('y_obs', mu=theta, sigma=sigma, observed=y)\n",
    "    \n",
    "    print(\"Model specification complete.\")\n",
    "    print()\n",
    "    print(\"Model structure:\")\n",
    "    print(pm.model_to_graphviz(hierarchical_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from posterior\n",
    "print(\"Sampling from posterior distribution...\")\n",
    "print(f\"Draws: {config.mcmc_draws}\")\n",
    "print(f\"Tune: {config.mcmc_tune}\")\n",
    "print(f\"Chains: {config.mcmc_chains}\")\n",
    "print()\n",
    "\n",
    "with hierarchical_model:\n",
    "    trace = pm.sample(\n",
    "        draws=config.mcmc_draws,\n",
    "        tune=config.mcmc_tune,\n",
    "        chains=config.mcmc_chains,\n",
    "        target_accept=config.mcmc_target_accept,\n",
    "        return_inferencedata=True,\n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "print(\"\\nâœ“ Sampling complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence diagnostics\n",
    "print(\"=== MCMC Convergence Diagnostics ===\")\n",
    "print()\n",
    "\n",
    "# R-hat (should be < 1.01)\n",
    "rhat = az.rhat(trace)\n",
    "print(\"R-hat statistics (Gelman-Rubin):\")\n",
    "for var in ['mu', 'sigma', 'sigma_textbook', 'sigma_discipline']:\n",
    "    if var in rhat:\n",
    "        val = float(rhat[var].values)\n",
    "        status = \"âœ“\" if val < 1.01 else \"âœ—\"\n",
    "        print(f\"  {var:<20}: {val:.4f} {status}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Effective sample size\n",
    "ess = az.ess(trace)\n",
    "print(\"Effective Sample Size (bulk):\")\n",
    "for var in ['mu', 'sigma', 'sigma_textbook', 'sigma_discipline']:\n",
    "    if var in ess:\n",
    "        val = float(ess[var].values)\n",
    "        status = \"âœ“\" if val > 400 else \"?\"\n",
    "        print(f\"  {var:<20}: {val:.0f} {status}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Plot traces\n",
    "az.plot_trace(\n",
    "    trace,\n",
    "    var_names=['mu', 'publisher_effect', 'sigma', 'sigma_textbook', 'sigma_discipline'],\n",
    "    compact=True,\n",
    "    figsize=(14, 10)\n",
    ")\n",
    "plt.suptitle('MCMC Trace Plots', fontsize=16, fontweight='bold', y=1.001)\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.figures_dir / 'mcmc_trace.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize posterior distributions\n",
    "print(\"=== Posterior Summary ===\")\n",
    "print()\n",
    "summary = az.summary(\n",
    "    trace,\n",
    "    var_names=['mu', 'publisher_effect', 'sigma', 'sigma_textbook', 'sigma_discipline'],\n",
    "    hdi_prob=0.95\n",
    ")\n",
    "display(summary)\n",
    "\n",
    "# Publisher effects with interpretations\n",
    "publisher_effects = trace.posterior['publisher_effect'].values\n",
    "publisher_effects_flat = publisher_effects.reshape(-1, n_publishers)\n",
    "\n",
    "print(\"\\n=== Publisher Type Effects ===\")\n",
    "print()\n",
    "print(f\"{'Publisher':<20} {'Mean':<10} {'SD':<10} {'95% HDI':<25} {'P(Î²>0)'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for pub_type, idx in publisher_map.items():\n",
    "    samples = publisher_effects_flat[:, idx]\n",
    "    mean = np.mean(samples)\n",
    "    sd = np.std(samples)\n",
    "    hdi = az.hdi(samples, hdi_prob=0.95)\n",
    "    prob_positive = np.mean(samples > 0)\n",
    "    \n",
    "    print(f\"{pub_type:<20} {mean:>9.3f} {sd:>9.3f} [{hdi[0]:>6.3f}, {hdi[1]:>6.3f}] {prob_positive:>8.3f}\")\n",
    "\n",
    "# Effect size interpretation\n",
    "forprofit_samples = publisher_effects_flat[:, 0]\n",
    "opensource_samples = publisher_effects_flat[:, 2]\n",
    "contrast = forprofit_samples - opensource_samples\n",
    "\n",
    "print(\"\\n=== Contrasts ===\")\n",
    "print()\n",
    "print(f\"For-Profit vs. Open-Source:\")\n",
    "print(f\"  Mean difference: {np.mean(contrast):.3f}\")\n",
    "print(f\"  95% HDI: [{az.hdi(contrast, hdi_prob=0.95)[0]:.3f}, {az.hdi(contrast, hdi_prob=0.95)[1]:.3f}]\")\n",
    "print(f\"  P(For-Profit > Open-Source): {np.mean(contrast > 0):.3f}\")\n",
    "print(f\"  P(|difference| > 0.5): {np.mean(np.abs(contrast) > 0.5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize posterior distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, (pub_type, pub_idx) in enumerate(publisher_map.items()):\n",
    "    samples = publisher_effects_flat[:, pub_idx]\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.hist(samples, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add mean and HDI\n",
    "    mean = np.mean(samples)\n",
    "    hdi = az.hdi(samples, hdi_prob=0.95)\n",
    "    \n",
    "    ax.axvline(mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean:.3f}')\n",
    "    ax.axvline(hdi[0], color='blue', linestyle=':', linewidth=1.5)\n",
    "    ax.axvline(hdi[1], color='blue', linestyle=':', linewidth=1.5, label=f'95% HDI')\n",
    "    ax.axvline(0, color='gray', linestyle='-', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    ax.set_title(pub_type, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Effect Size', fontsize=10)\n",
    "    ax.set_ylabel('Density', fontsize=10)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Posterior Distributions of Publisher Effects', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.figures_dir / 'publisher_effects_posterior.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Visualization and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results by publisher type\n",
    "publisher_summary = df.groupby('publisher_type').agg({\n",
    "    target_factor: ['mean', 'std', 'count'],\n",
    "    'textbook_id': 'nunique'\n",
    "}).round(3)\n",
    "\n",
    "print(\"=== Descriptive Statistics by Publisher Type ===\")\n",
    "print()\n",
    "display(publisher_summary)\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Box plot\n",
    "ax1 = axes[0, 0]\n",
    "df.boxplot(column=target_factor, by='publisher_type', ax=ax1)\n",
    "ax1.set_title('Distribution by Publisher Type', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Publisher Type')\n",
    "ax1.set_ylabel(target_factor)\n",
    "plt.sca(ax1)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Violin plot\n",
    "ax2 = axes[0, 1]\n",
    "publisher_order = ['For-Profit', 'University Press', 'Open-Source']\n",
    "sns.violinplot(data=df, x='publisher_type', y=target_factor, \n",
    "               order=publisher_order, ax=ax2)\n",
    "ax2.set_title('Density Distribution', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Publisher Type')\n",
    "ax2.set_ylabel(target_factor)\n",
    "plt.sca(ax2)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# By discipline\n",
    "ax3 = axes[1, 0]\n",
    "discipline_summary = df.groupby(['discipline', 'publisher_type'])[target_factor].mean().unstack()\n",
    "discipline_summary.plot(kind='bar', ax=ax3)\n",
    "ax3.set_title('Mean Factor Score by Discipline', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Discipline')\n",
    "ax3.set_ylabel(f'Mean {target_factor}')\n",
    "ax3.legend(title='Publisher', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.sca(ax3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Posterior effect sizes\n",
    "ax4 = axes[1, 1]\n",
    "for pub_type, idx in publisher_map.items():\n",
    "    samples = publisher_effects_flat[:, idx]\n",
    "    ax4.hist(samples, bins=30, alpha=0.5, label=pub_type, density=True)\n",
    "ax4.axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax4.set_title('Posterior Publisher Effects', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Effect Size')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.figures_dir / 'comprehensive_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "print(\"Exporting results...\")\n",
    "print()\n",
    "\n",
    "# 1. Factor analysis results\n",
    "loadings.to_csv(config.results_dir / 'factor_loadings.csv')\n",
    "print(\"âœ“ Factor loadings saved\")\n",
    "\n",
    "# 2. Reliability results\n",
    "reliability_df = pd.DataFrame({\n",
    "    'Dimension': list(reliability_results.keys()),\n",
    "    'Krippendorff_Alpha': list(reliability_results.values())\n",
    "})\n",
    "reliability_df.to_csv(config.results_dir / 'reliability.csv', index=False)\n",
    "print(\"âœ“ Reliability results saved\")\n",
    "\n",
    "# 3. Posterior summary\n",
    "summary.to_csv(config.results_dir / 'posterior_summary.csv')\n",
    "print(\"âœ“ Posterior summary saved\")\n",
    "\n",
    "# 4. Publisher effects\n",
    "publisher_effects_summary = pd.DataFrame({\n",
    "    'Publisher': list(publisher_map.keys()),\n",
    "    'Mean_Effect': [np.mean(publisher_effects_flat[:, idx]) for idx in publisher_map.values()],\n",
    "    'SD_Effect': [np.std(publisher_effects_flat[:, idx]) for idx in publisher_map.values()],\n",
    "    'HDI_Lower': [az.hdi(publisher_effects_flat[:, idx], hdi_prob=0.95)[0] for idx in publisher_map.values()],\n",
    "    'HDI_Upper': [az.hdi(publisher_effects_flat[:, idx], hdi_prob=0.95)[1] for idx in publisher_map.values()]\n",
    "})\n",
    "publisher_effects_summary.to_csv(config.results_dir / 'publisher_effects.csv', index=False)\n",
    "print(\"âœ“ Publisher effects saved\")\n",
    "\n",
    "# 5. Full dataset with factor scores\n",
    "df.to_csv(config.results_dir / 'complete_dataset.csv', index=False)\n",
    "print(\"âœ“ Complete dataset saved\")\n",
    "\n",
    "# 6. Trace data\n",
    "trace.to_netcdf(config.results_dir / 'mcmc_trace.nc')\n",
    "print(\"âœ“ MCMC trace saved\")\n",
    "\n",
    "print()\n",
    "print(f\"All results exported to: {config.results_dir}\")\n",
    "print(f\"All figures saved to: {config.figures_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEXTBOOK BIAS DETECTION PROJECT - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"ðŸ“Š DATA\")\n",
    "print(f\"  â€¢ Textbooks analyzed: {df['textbook_id'].nunique()}\")\n",
    "print(f\"  â€¢ Passages rated: {len(df)}\")\n",
    "print(f\"  â€¢ Publisher types: {df['publisher_type'].nunique()}\")\n",
    "print(f\"  â€¢ Disciplines: {df['discipline'].nunique()}\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸ¤– LLM ENSEMBLE\")\n",
    "print(f\"  â€¢ Models: {', '.join(config.llm_models)}\")\n",
    "print(f\"  â€¢ Bias dimensions: {len(config.bias_dimensions)}\")\n",
    "print(f\"  â€¢ Overall reliability (Î±): {overall_alpha:.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸ“ˆ FACTOR ANALYSIS\")\n",
    "print(f\"  â€¢ Factors extracted: {n_factors}\")\n",
    "print(f\"  â€¢ Variance explained: {total_var_explained:.1f}%\")\n",
    "print(f\"  â€¢ KMO adequacy: {kmo_model:.3f}\")\n",
    "print(f\"  â€¢ Bartlett's test: p < 0.001\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸŽ¯ BAYESIAN INFERENCE\")\n",
    "print(f\"  â€¢ MCMC samples: {config.mcmc_draws} Ã— {config.mcmc_chains} chains\")\n",
    "print(f\"  â€¢ All RÌ‚ < 1.01: âœ“\")\n",
    "print(f\"  â€¢ Effective sample size: adequate\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸ’¡ KEY FINDINGS\")\n",
    "print(f\"  â€¢ For-Profit vs Open-Source effect: {np.mean(contrast):.3f}\")\n",
    "print(f\"  â€¢ P(For-Profit > Open-Source): {np.mean(contrast > 0):.3f}\")\n",
    "print(f\"  â€¢ Effect is credible: {'âœ“' if np.mean(np.abs(contrast) > 0.3) > 0.95 else '?'}\")\n",
    "print()\n",
    "\n",
    "print(\"âœ… PROJECT COMPLETE\")\n",
    "print(f\"  â€¢ Results: {config.results_dir}\")\n",
    "print(f\"  â€¢ Figures: {config.figures_dir}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
