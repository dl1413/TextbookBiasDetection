{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Publisher Bias in Academic Textbooks\n",
    "## Using Bayesian Ensemble Methods and Large Language Models\n",
    "\n",
    "**Author:** Derek Lankeaux  \n",
    "**Institution:** Rochester Institute of Technology  \n",
    "**Project:** MS Applied Statistics \u2013 Capstone Project  \n",
    "**Date:** November 18, 2025\n",
    "\n",
    "---\n",
    "\n",
    "### Abstract\n",
    "\n",
    "This notebook implements a comprehensive methodological framework for detecting and quantifying publisher bias in academic textbooks. We use:\n",
    "\n",
    "1. **LLM Ensemble Rating System** - GPT-4, Claude-3, and Llama-3 for multi-dimensional bias assessment\n",
    "2. **Exploratory Factor Analysis** - Uncover latent bias dimensions with varimax rotation\n",
    "3. **Bayesian Hierarchical Models** - Quantify publisher-type effects using PyMC\n",
    "4. **Validation Studies** - Inter-rater reliability and convergent validity testing\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "- **RQ1:** What latent dimensions underlie systematic variation in textbook content?\n",
    "- **RQ2:** Do publisher types (for-profit, university press, open-source) differ systematically?\n",
    "- **RQ3:** Do publisher effects vary across disciplines?\n",
    "- **RQ4:** Do LLM ensemble ratings demonstrate acceptable validity and reliability?\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- **150 textbooks** stratified across:\n",
    "  - 3 publisher types (For-Profit: n=75, University Press: n=50, Open-Source: n=25)\n",
    "  - 6 disciplines (Biology, Chemistry, Computer Science, Economics, Psychology, History)\n",
    "- **4,500 passages** (30 per textbook)\n",
    "- **5 rating dimensions:** Perspective Balance, Source Authority, Commercial Framing, Certainty Language, Ideological Framing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import bartlett, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Factor Analysis\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "\n",
    "# Bayesian modeling\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# LLM API integrations\n",
    "import openai\n",
    "import anthropic\n",
    "# Note: For Llama-3, we would use together.ai or replicate API\n",
    "\n",
    "# Reliability metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import krippendorff\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\u2713 All libraries imported successfully\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"PyMC version: {pm.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "PUBLISHER_TYPES = ['For-Profit', 'University Press', 'Open-Source']\n",
    "DISCIPLINES = ['Biology', 'Chemistry', 'Computer Science', 'Economics', 'Psychology', 'History']\n",
    "\n",
    "# Sample sizes\n",
    "N_TEXTBOOKS = {'For-Profit': 75, 'University Press': 50, 'Open-Source': 25}\n",
    "PASSAGES_PER_BOOK = 30\n",
    "TOTAL_PASSAGES = 4500\n",
    "\n",
    "# Rating dimensions\n",
    "RATING_DIMENSIONS = [\n",
    "    'Perspective_Balance',\n",
    "    'Source_Authority', \n",
    "    'Commercial_Framing',\n",
    "    'Certainty_Language',\n",
    "    'Ideological_Framing'\n",
    "]\n",
    "\n",
    "# LLM models\n",
    "LLM_MODELS = ['GPT-4', 'Claude-3', 'Llama-3']\n",
    "\n",
    "# Rating scale\n",
    "RATING_SCALE = (1, 7)  # 7-point Likert scale\n",
    "\n",
    "# File paths\n",
    "DATA_DIR = Path('data')\n",
    "RESULTS_DIR = Path('results')\n",
    "FIGURES_DIR = Path('figures')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [DATA_DIR, RESULTS_DIR, FIGURES_DIR]:\n",
    "    directory.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\u2713 Configuration complete\")\n",
    "print(f\"Expected total passages: {TOTAL_PASSAGES}\")\n",
    "print(f\"Rating dimensions: {len(RATING_DIMENSIONS)}\")\n",
    "print(f\"LLM models: {len(LLM_MODELS)}\")\n",
    "print(f\"Total features per passage: {len(RATING_DIMENSIONS) * len(LLM_MODELS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Generation (Simulated Dataset)\n",
    "\n",
    "For demonstration purposes, we generate a synthetic dataset that mirrors the expected structure. In production, this would load real textbook passages and LLM ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_dataset(seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic textbook bias dataset with realistic patterns.\n",
    "    \n",
    "    This simulates:\n",
    "    - 150 textbooks across 3 publisher types and 6 disciplines\n",
    "    - 30 passages per textbook (4,500 total)\n",
    "    - Ratings from 3 LLM models on 5 dimensions\n",
    "    - Publisher-specific bias patterns\n",
    "    \n",
    "    OPTIMIZED: Uses vectorized NumPy operations instead of nested loops\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Define publisher effects (ground truth for validation)\n",
    "    publisher_effects = {\n",
    "        'For-Profit': {\n",
    "            'Commercial_Framing': 0.8,      # Higher commercial framing\n",
    "            'Perspective_Balance': -0.6,    # Lower perspective diversity\n",
    "            'Source_Authority': 0.3,\n",
    "            'Certainty_Language': 0.4,\n",
    "            'Ideological_Framing': 0.2\n",
    "        },\n",
    "        'University Press': {\n",
    "            'Commercial_Framing': 0.0,      # Baseline\n",
    "            'Perspective_Balance': 0.0,\n",
    "            'Source_Authority': 0.0,\n",
    "            'Certainty_Language': 0.0,\n",
    "            'Ideological_Framing': 0.0\n",
    "        },\n",
    "        'Open-Source': {\n",
    "            'Commercial_Framing': -0.7,     # Lower commercial framing\n",
    "            'Perspective_Balance': 0.6,     # Higher perspective diversity\n",
    "            'Source_Authority': -0.2,\n",
    "            'Certainty_Language': -0.3,\n",
    "            'Ideological_Framing': 0.4\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # OPTIMIZATION: Pre-allocate arrays for vectorized operations\n",
    "    total_passages = TOTAL_PASSAGES\n",
    "    \n",
    "    # Create base structure arrays\n",
    "    textbook_ids = []\n",
    "    publisher_types = []\n",
    "    disciplines_list = []\n",
    "    \n",
    "    textbook_id = 0\n",
    "    for publisher_type in PUBLISHER_TYPES:\n",
    "        n_books = N_TEXTBOOKS[publisher_type]\n",
    "        for _ in range(n_books):\n",
    "            textbook_id += 1\n",
    "            # Each textbook has PASSAGES_PER_BOOK passages\n",
    "            textbook_ids.extend([textbook_id] * PASSAGES_PER_BOOK)\n",
    "            publisher_types.extend([publisher_type] * PASSAGES_PER_BOOK)\n",
    "            # Randomly assign discipline per textbook\n",
    "            discipline = np.random.choice(DISCIPLINES)\n",
    "            disciplines_list.extend([discipline] * PASSAGES_PER_BOOK)\n",
    "    \n",
    "    # Create base DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'textbook_id': textbook_ids,\n",
    "        'publisher_type': publisher_types,\n",
    "        'discipline': disciplines_list\n",
    "    })\n",
    "    \n",
    "    # Generate passage IDs vectorized\n",
    "    df['passage_id'] = df.apply(lambda x: f\"T{x['textbook_id']}_P{x.name % PASSAGES_PER_BOOK + 1}\", axis=1)\n",
    "    \n",
    "    # OPTIMIZATION: Vectorized rating generation\n",
    "    # Generate discipline effects once per textbook\n",
    "    discipline_effects = np.random.normal(0, 0.2, size=len(df))\n",
    "    \n",
    "    # Generate ratings for all dimensions and models\n",
    "    base_rating = 4.0\n",
    "    \n",
    "    for dimension in RATING_DIMENSIONS:\n",
    "        # Get publisher effects for this dimension vectorized\n",
    "        publisher_effect_map = {pub: publisher_effects[pub][dimension] for pub in PUBLISHER_TYPES}\n",
    "        dimension_publisher_effects = df['publisher_type'].map(publisher_effect_map).values\n",
    "        \n",
    "        # Calculate true ratings for all passages at once\n",
    "        true_ratings = base_rating + dimension_publisher_effects + discipline_effects\n",
    "        \n",
    "        # Generate model ratings with noise\n",
    "        for model in LLM_MODELS:\n",
    "            # Vectorized noise generation\n",
    "            model_noise = np.random.normal(0, 0.3, size=len(df))\n",
    "            ratings = np.clip(true_ratings + model_noise, 1, 7)\n",
    "            \n",
    "            # Store in dataframe\n",
    "            column_name = f'{dimension}_{model.replace(\"-\", \"_\")}'\n",
    "            df[column_name] = ratings\n",
    "    \n",
    "    print(f\"\u2713 Generated dataset with {len(df)} passages (OPTIMIZED)\")\n",
    "    print(f\"  - Textbooks: {df['textbook_id'].nunique()}\")\n",
    "    print(f\"  - Publisher types: {df['publisher_type'].nunique()}\")\n",
    "    print(f\"  - Disciplines: {df['discipline'].nunique()}\")\n",
    "    print(f\"  - Features: {len(df.columns)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset:\n",
    "    1. Extract rating columns\n",
    "    2. Calculate consensus ratings (mean across LLM models)\n",
    "    3. Standardize ratings\n",
    "    \n",
    "    OPTIMIZED: Vectorized operations for consensus calculation\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # OPTIMIZATION: Vectorized consensus calculation using regex column selection\n",
    "    # Instead of looping through dimensions, select all model columns at once and compute\n",
    "    for dimension in RATING_DIMENSIONS:\n",
    "        # Use list comprehension but optimize the pattern matching\n",
    "        model_cols = [f'{dimension}_{model.replace(\"-\", \"_\")}' for model in LLM_MODELS]\n",
    "        # Vectorized mean calculation\n",
    "        df_processed[f'{dimension}_consensus'] = df_processed[model_cols].mean(axis=1)\n",
    "    \n",
    "    print(\"\u2713 Calculated consensus ratings (OPTIMIZED)\")\n",
    "    \n",
    "    # OPTIMIZATION: Use regex for column selection - more efficient\n",
    "    import re\n",
    "    rating_pattern = re.compile('|'.join([dim for dim in RATING_DIMENSIONS]))\n",
    "    model_pattern = re.compile('GPT|Claude|Llama')\n",
    "    rating_cols = [col for col in df_processed.columns \n",
    "                   if rating_pattern.search(col) and model_pattern.search(col)]\n",
    "    X_ratings = df_processed[rating_cols].values\n",
    "    \n",
    "    print(f\"\u2713 Extracted rating matrix: {X_ratings.shape}\")\n",
    "    \n",
    "    return df_processed, X_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inter-Rater Reliability Analysis\n",
    "\n",
    "Calculate Krippendorff's Alpha to assess agreement between LLM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_inter_rater_reliability(df):\n",
    "    \"\"\"\n",
    "    Calculate Krippendorff's Alpha for each rating dimension.\n",
    "    \"\"\"\n",
    "    reliability_results = {}\n",
    "    \n",
    "    for dimension in RATING_DIMENSIONS:\n",
    "        # Extract ratings from all 3 models for this dimension\n",
    "        model_cols = [f'{dimension}_{model.replace(\"-\", \"_\")}' for model in LLM_MODELS]\n",
    "        ratings_matrix = df[model_cols].values.T  # Shape: (n_raters, n_items)\n",
    "        \n",
    "        # Calculate Krippendorff's Alpha\n",
    "        alpha = krippendorff.alpha(reliability_data=ratings_matrix, level_of_measurement='interval')\n",
    "        reliability_results[dimension] = alpha\n",
    "    \n",
    "    return reliability_results\n",
    "\n",
    "reliability_scores = calculate_inter_rater_reliability(df_processed)\n",
    "\n",
    "print(\"Inter-Rater Reliability (Krippendorff's Alpha):\")\n",
    "print(\"=\"*60)\n",
    "for dimension, alpha in reliability_scores.items():\n",
    "    interpretation = \"Excellent\" if alpha >= 0.80 else \"Good\" if alpha >= 0.70 else \"Acceptable\" if alpha >= 0.60 else \"Questionable\"\n",
    "    print(f\"{dimension:25s}: \u03b1 = {alpha:.3f} ({interpretation})\")\n",
    "\n",
    "overall_alpha = np.mean(list(reliability_scores.values()))\n",
    "print(f\"\\nOverall Mean Alpha: {overall_alpha:.3f}\")\n",
    "\n",
    "# Visualize reliability scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "dimensions_short = [d.replace('_', ' ') for d in RATING_DIMENSIONS]\n",
    "alphas = list(reliability_scores.values())\n",
    "\n",
    "bars = plt.bar(dimensions_short, alphas, color=['green' if a >= 0.80 else 'orange' if a >= 0.70 else 'red' for a in alphas])\n",
    "plt.axhline(y=0.80, color='green', linestyle='--', label='Excellent (\u03b1 \u2265 0.80)', alpha=0.5)\n",
    "plt.axhline(y=0.70, color='orange', linestyle='--', label='Good (\u03b1 \u2265 0.70)', alpha=0.5)\n",
    "plt.xlabel('Rating Dimension')\n",
    "plt.ylabel(\"Krippendorff's Alpha\")\n",
    "plt.title('Inter-Rater Reliability Across Dimensions')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'inter_rater_reliability.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\u2713 Reliability analysis plot saved to {FIGURES_DIR / 'inter_rater_reliability.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Factor Analysis (EFA)\n",
    "\n",
    "### 6.1 Factorability Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use consensus ratings for factor analysis\n",
    "consensus_cols = [f'{dim}_consensus' for dim in RATING_DIMENSIONS]\n",
    "X_factor = df_processed[consensus_cols].values\n",
    "\n",
    "# Bartlett's Test of Sphericity\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(X_factor)\n",
    "print(\"Bartlett's Test of Sphericity:\")\n",
    "print(f\"  Chi-square statistic: {chi_square_value:.2f}\")\n",
    "print(f\"  p-value: {p_value:.2e}\")\n",
    "print(f\"  Result: {'Reject H0 - Data suitable for FA' if p_value < 0.05 else 'Fail to reject H0'}\")\n",
    "\n",
    "# Kaiser-Meyer-Olkin (KMO) Test\n",
    "kmo_all, kmo_model = calculate_kmo(X_factor)\n",
    "print(f\"\\nKaiser-Meyer-Olkin (KMO) Measure:\")\n",
    "print(f\"  Overall KMO: {kmo_model:.3f}\")\n",
    "\n",
    "interpretation = (\n",
    "    \"Marvelous\" if kmo_model >= 0.90 else\n",
    "    \"Meritorious\" if kmo_model >= 0.80 else\n",
    "    \"Middling\" if kmo_model >= 0.70 else\n",
    "    \"Mediocre\" if kmo_model >= 0.60 else\n",
    "    \"Miserable\"\n",
    ")\n",
    "print(f\"  Interpretation: {interpretation}\")\n",
    "\n",
    "print(\"\\nVariable-specific KMO:\")\n",
    "for i, dim in enumerate(RATING_DIMENSIONS):\n",
    "    print(f\"  {dim:25s}: {kmo_all[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Determine Number of Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot and parallel analysis\n",
    "fa_initial = FactorAnalyzer(n_factors=len(RATING_DIMENSIONS), rotation=None)\n",
    "fa_initial.fit(X_factor)\n",
    "\n",
    "eigenvalues, _ = fa_initial.get_eigenvalues()\n",
    "\n",
    "# OPTIMIZATION: Parallel analysis using multiprocessing for faster execution\n",
    "n_iterations = 100\n",
    "\n",
    "def compute_random_eigenvalues(iteration):\n",
    "    \"\"\"Helper function for parallel eigenvalue computation\"\"\"\n",
    "    random_data = np.random.normal(size=X_factor.shape)\n",
    "    fa_random = FactorAnalyzer(n_factors=len(RATING_DIMENSIONS), rotation=None)\n",
    "    fa_random.fit(random_data)\n",
    "    ev_random, _ = fa_random.get_eigenvalues()\n",
    "    return ev_random\n",
    "\n",
    "print(f\"Computing parallel analysis with {n_iterations} iterations...\")\n",
    "\n",
    "# Use multiprocessing for parallel execution\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "\n",
    "# Determine number of workers (use all but one CPU)\n",
    "n_workers = max(1, cpu_count() - 1)\n",
    "\n",
    "with Pool(processes=n_workers) as pool:\n",
    "    random_eigenvalues = pool.map(compute_random_eigenvalues, range(n_iterations))\n",
    "\n",
    "random_eigenvalues_mean = np.mean(random_eigenvalues, axis=0)\n",
    "print(f\"\u2713 Parallel analysis complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Factor Extraction with Varimax Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit factor analysis with optimal number of factors\n",
    "n_factors = 4  # Based on theoretical expectations and parallel analysis\n",
    "\n",
    "fa = FactorAnalyzer(n_factors=n_factors, rotation='varimax', method='principal')\n",
    "fa.fit(X_factor)\n",
    "\n",
    "# Get factor loadings\n",
    "loadings = fa.loadings_\n",
    "loadings_df = pd.DataFrame(\n",
    "    loadings,\n",
    "    index=RATING_DIMENSIONS,\n",
    "    columns=[f'Factor {i+1}' for i in range(n_factors)]\n",
    ")\n",
    "\n",
    "print(\"Factor Loadings (Varimax Rotated):\")\n",
    "print(\"=\"*80)\n",
    "print(loadings_df.round(3))\n",
    "\n",
    "# Get variance explained\n",
    "variance = fa.get_factor_variance()\n",
    "variance_df = pd.DataFrame(\n",
    "    variance,\n",
    "    index=['Variance', 'Proportional Var', 'Cumulative Var'],\n",
    "    columns=[f'Factor {i+1}' for i in range(n_factors)]\n",
    ")\n",
    "\n",
    "print(\"\\nVariance Explained:\")\n",
    "print(\"=\"*80)\n",
    "print(variance_df.round(3))\n",
    "print(f\"\\nTotal variance explained: {variance_df.loc['Cumulative Var'].iloc[-1]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Visualize Factor Loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of factor loadings\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(loadings_df, annot=True, cmap='RdBu_r', center=0, vmin=-1, vmax=1, \n",
    "            cbar_kws={'label': 'Loading'}, fmt='.2f')\n",
    "plt.title('Factor Loading Matrix (Varimax Rotated)')\n",
    "plt.ylabel('Rating Dimension')\n",
    "plt.xlabel('Latent Factor')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'factor_loadings_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Interpret factors based on loadings\n",
    "print(\"\\nFactor Interpretation (based on highest loadings):\")\n",
    "print(\"=\"*80)\n",
    "factor_interpretations = {\n",
    "    'Factor 1': 'Commercial Influence',\n",
    "    'Factor 2': 'Perspective Diversity', \n",
    "    'Factor 3': 'Epistemic Certainty',\n",
    "    'Factor 4': 'Political Framing'\n",
    "}\n",
    "\n",
    "for factor, interpretation in factor_interpretations.items():\n",
    "    print(f\"{factor}: {interpretation}\")\n",
    "    top_loadings = loadings_df[factor].abs().sort_values(ascending=False).head(3)\n",
    "    for dim, loading in top_loadings.items():\n",
    "        print(f\"  - {dim}: {loadings_df.loc[dim, factor]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Calculate Factor Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate factor scores for each passage\n",
    "factor_scores = fa.transform(X_factor)\n",
    "\n",
    "# Add factor scores to dataframe\n",
    "for i in range(n_factors):\n",
    "    df_processed[f'factor_{i+1}_score'] = factor_scores[:, i]\n",
    "\n",
    "# Add interpretable names\n",
    "df_processed['Commercial_Influence'] = df_processed['factor_1_score']\n",
    "df_processed['Perspective_Diversity'] = df_processed['factor_2_score']\n",
    "df_processed['Epistemic_Certainty'] = df_processed['factor_3_score']\n",
    "df_processed['Political_Framing'] = df_processed['factor_4_score']\n",
    "\n",
    "print(\"\u2713 Factor scores calculated and added to dataset\")\n",
    "print(\"\\nFactor Score Summary Statistics:\")\n",
    "factor_score_cols = ['Commercial_Influence', 'Perspective_Diversity', 'Epistemic_Certainty', 'Political_Framing']\n",
    "df_processed[factor_score_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Descriptive Statistics by Publisher Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by publisher type and calculate means\n",
    "publisher_summary = df_processed.groupby('publisher_type')[factor_score_cols].agg(['mean', 'std', 'count'])\n",
    "\n",
    "print(\"Factor Scores by Publisher Type:\")\n",
    "print(\"=\"*100)\n",
    "print(publisher_summary)\n",
    "\n",
    "# Visualize factor scores by publisher type\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, factor in enumerate(factor_score_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Box plot\n",
    "    df_processed.boxplot(column=factor, by='publisher_type', ax=ax)\n",
    "    ax.set_title(f'{factor.replace(\"_\", \" \")}')\n",
    "    ax.set_xlabel('Publisher Type')\n",
    "    ax.set_ylabel('Factor Score')\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.suptitle('Factor Scores by Publisher Type', fontsize=16, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'factor_scores_by_publisher.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bayesian Hierarchical Models\n",
    "\n",
    "### 8.1 Model 1: Publisher Type Effects on Commercial Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyMC\n",
    "# Encode publisher type\n",
    "publisher_mapping = {pub: idx for idx, pub in enumerate(PUBLISHER_TYPES)}\n",
    "df_processed['publisher_idx'] = df_processed['publisher_type'].map(publisher_mapping)\n",
    "\n",
    "# Encode discipline\n",
    "discipline_mapping = {disc: idx for idx, disc in enumerate(DISCIPLINES)}\n",
    "df_processed['discipline_idx'] = df_processed['discipline'].map(discipline_mapping)\n",
    "\n",
    "# Extract variables\n",
    "y_commercial = df_processed['Commercial_Influence'].values\n",
    "publisher_idx = df_processed['publisher_idx'].values\n",
    "discipline_idx = df_processed['discipline_idx'].values\n",
    "textbook_idx = df_processed['textbook_id'].values - 1  # 0-indexed\n",
    "\n",
    "n_publishers = len(PUBLISHER_TYPES)\n",
    "n_disciplines = len(DISCIPLINES)\n",
    "n_textbooks = df_processed['textbook_id'].nunique()\n",
    "\n",
    "print(f\"Model setup:\")\n",
    "print(f\"  Observations: {len(y_commercial)}\")\n",
    "print(f\"  Publishers: {n_publishers}\")\n",
    "print(f\"  Disciplines: {n_disciplines}\")\n",
    "print(f\"  Textbooks: {n_textbooks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Bayesian hierarchical model\n",
    "with pm.Model() as model_commercial:\n",
    "    # Priors for fixed effects (publisher type)\n",
    "    # Use University Press as reference category (effect = 0)\n",
    "    beta_forprofit = pm.Normal('beta_ForProfit', mu=0, sigma=1)\n",
    "    beta_opensource = pm.Normal('beta_OpenSource', mu=0, sigma=1)\n",
    "    \n",
    "    # Combine into effect array (University Press = 0 by construction)\n",
    "    publisher_effects = pm.math.stack([beta_forprofit, 0, beta_opensource])\n",
    "    \n",
    "    # Priors for random effects\n",
    "    # Discipline-level random effects\n",
    "    sigma_discipline = pm.HalfNormal('sigma_discipline', sigma=0.5)\n",
    "    discipline_effects = pm.Normal('discipline_effects', mu=0, sigma=sigma_discipline, shape=n_disciplines)\n",
    "    \n",
    "    # Textbook-level random effects\n",
    "    sigma_textbook = pm.HalfNormal('sigma_textbook', sigma=0.5)\n",
    "    textbook_effects = pm.Normal('textbook_effects', mu=0, sigma=sigma_textbook, shape=n_textbooks)\n",
    "    \n",
    "    # Overall intercept\n",
    "    alpha = pm.Normal('alpha', mu=0, sigma=1)\n",
    "    \n",
    "    # Linear predictor\n",
    "    mu = alpha + publisher_effects[publisher_idx] + discipline_effects[discipline_idx] + textbook_effects[textbook_idx]\n",
    "    \n",
    "    # Likelihood\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y_commercial)\n",
    "    \n",
    "    # Sample from posterior\n",
    "    trace_commercial = pm.sample(2000, tune=1000, return_inferencedata=True, random_seed=42, \n",
    "                                 target_accept=0.95, chains=4)\n",
    "\n",
    "print(\"\u2713 Model 1 sampling complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check convergence diagnostics\n",
    "print(\"Convergence Diagnostics (R-hat):\")\n",
    "print(\"=\"*60)\n",
    "summary = az.summary(trace_commercial, var_names=['beta_ForProfit', 'beta_OpenSource', 'sigma_discipline', 'sigma_textbook'])\n",
    "print(summary)\n",
    "\n",
    "# Plot trace plots\n",
    "az.plot_trace(trace_commercial, var_names=['beta_ForProfit', 'beta_OpenSource', 'alpha'], \n",
    "              compact=True, figsize=(12, 8))\n",
    "plt.suptitle('Trace Plots - Commercial Influence Model', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'trace_commercial_influence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Forest plot of publisher effects\n",
    "az.plot_forest(trace_commercial, var_names=['beta_ForProfit', 'beta_OpenSource'],\n",
    "               combined=True, figsize=(10, 4), hdi_prob=0.95)\n",
    "plt.title('Publisher Type Effects on Commercial Influence\\n(95% Credible Intervals)')\n",
    "plt.xlabel('Effect Size (University Press = baseline)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'forest_commercial_influence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Posterior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples\n",
    "posterior_forprofit = trace_commercial.posterior['beta_ForProfit'].values.flatten()\n",
    "posterior_opensource = trace_commercial.posterior['beta_OpenSource'].values.flatten()\n",
    "\n",
    "# Calculate posterior summaries\n",
    "print(\"Posterior Summaries (Commercial Influence):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"For-Profit vs University Press:\")\n",
    "print(f\"  Mean: {posterior_forprofit.mean():.3f}\")\n",
    "print(f\"  95% CI: [{np.percentile(posterior_forprofit, 2.5):.3f}, {np.percentile(posterior_forprofit, 97.5):.3f}]\")\n",
    "print(f\"  P(effect > 0): {(posterior_forprofit > 0).mean():.3f}\")\n",
    "\n",
    "print(f\"\\nOpen-Source vs University Press:\")\n",
    "print(f\"  Mean: {posterior_opensource.mean():.3f}\")\n",
    "print(f\"  95% CI: [{np.percentile(posterior_opensource, 2.5):.3f}, {np.percentile(posterior_opensource, 97.5):.3f}]\")\n",
    "print(f\"  P(effect < 0): {(posterior_opensource < 0).mean():.3f}\")\n",
    "\n",
    "# Calculate effect size (Cohen's d)\n",
    "pooled_std = trace_commercial.posterior['sigma'].values.flatten().mean()\n",
    "cohens_d_forprofit = posterior_forprofit.mean() / pooled_std\n",
    "cohens_d_opensource = posterior_opensource.mean() / pooled_std\n",
    "\n",
    "print(f\"\\nEffect Sizes (Cohen's d):\")\n",
    "print(f\"  For-Profit: {cohens_d_forprofit:.3f}\")\n",
    "print(f\"  Open-Source: {cohens_d_opensource:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Model 2: All Four Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models for all four factors\n",
    "factor_models = {}\n",
    "factor_traces = {}\n",
    "\n",
    "print(\"\\nFitting Bayesian models for all factors...\")\n",
    "print(\"Note: Using reduced sampling for demonstration (increase for production)\")\n",
    "\n",
    "for factor_name in factor_score_cols:\n",
    "    print(f\"\\n  \u2192 Fitting model for {factor_name}...\")\n",
    "    \n",
    "    y = df_processed[factor_name].values\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        # Publisher effects (University Press = baseline)\n",
    "        beta_forprofit = pm.Normal(f'beta_ForProfit', mu=0, sigma=1)\n",
    "        beta_opensource = pm.Normal(f'beta_OpenSource', mu=0, sigma=1)\n",
    "        publisher_effects = pm.math.stack([beta_forprofit, 0, beta_opensource])\n",
    "        \n",
    "        # Random effects\n",
    "        sigma_discipline = pm.HalfNormal('sigma_discipline', sigma=0.5)\n",
    "        discipline_effects = pm.Normal('discipline_effects', mu=0, sigma=sigma_discipline, shape=n_disciplines)\n",
    "        \n",
    "        sigma_textbook = pm.HalfNormal('sigma_textbook', sigma=0.5)\n",
    "        textbook_effects = pm.Normal('textbook_effects', mu=0, sigma=sigma_textbook, shape=n_textbooks)\n",
    "        \n",
    "        # Intercept and linear predictor\n",
    "        alpha = pm.Normal('alpha', mu=0, sigma=1)\n",
    "        mu = alpha + publisher_effects[publisher_idx] + discipline_effects[discipline_idx] + textbook_effects[textbook_idx]\n",
    "        \n",
    "        # Likelihood\n",
    "        sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "        y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)\n",
    "        \n",
    "        # OPTIMIZATION: Use progressbar=True for user feedback during sampling\n",
    "        # Sample with reduced iterations for demonstration (increase for production)\n",
    "        trace = pm.sample(1000, tune=500, return_inferencedata=True, random_seed=42,\n",
    "                         target_accept=0.90, chains=2, progressbar=True)\n",
    "    \n",
    "    factor_models[factor_name] = model\n",
    "    factor_traces[factor_name] = trace\n",
    "    print(f\"    \u2713 Complete\")\n",
    "    \n",
    "    # OPTIMIZATION: Memory management - clear large temporary variables\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\u2713 All factor models fitted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Comprehensive Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"\\nBayesian Hierarchical Model Results\")\n",
    "print(\"Publisher Type Effects (University Press = baseline)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# OPTIMIZATION: Replace iterrows() with vectorized string formatting\n",
    "# iterrows() is ~100x slower than vectorized operations\n",
    "result_strings = (\n",
    "    results_df['Factor'].str.ljust(25) + ' | ' +\n",
    "    results_df['Publisher'].str.ljust(15) + ' | ' +\n",
    "    '\u03b2 = ' + results_df['Mean'].map('{:6.3f}'.format) + \n",
    "    ' [' + results_df['CI_Lower'].map('{:6.3f}'.format) + \n",
    "    ', ' + results_df['CI_Upper'].map('{:6.3f}'.format) + '] | ' +\n",
    "    'P = ' + results_df['P(Direction)'].map('{:.3f}'.format)\n",
    ")\n",
    "\n",
    "for result_str in result_strings:\n",
    "    print(result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comprehensive Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive forest plot for all factors\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, factor_name in enumerate(factor_score_cols):\n",
    "    ax = axes[idx]\n",
    "    trace = factor_traces[factor_name]\n",
    "    \n",
    "    # Extract posteriors\n",
    "    fp_posterior = trace.posterior['beta_ForProfit'].values.flatten()\n",
    "    os_posterior = trace.posterior['beta_OpenSource'].values.flatten()\n",
    "    \n",
    "    # Calculate HDI\n",
    "    fp_hdi = az.hdi(fp_posterior, hdi_prob=0.95)\n",
    "    os_hdi = az.hdi(os_posterior, hdi_prob=0.95)\n",
    "    \n",
    "    # Plot\n",
    "    y_positions = [1, 0]\n",
    "    means = [fp_posterior.mean(), os_posterior.mean()]\n",
    "    hdis = [fp_hdi, os_hdi]\n",
    "    labels = ['For-Profit', 'Open-Source']\n",
    "    colors = ['#e74c3c', '#3498db']\n",
    "    \n",
    "    for y, mean, hdi, label, color in zip(y_positions, means, hdis, labels, colors):\n",
    "        ax.plot([hdi[0], hdi[1]], [y, y], linewidth=3, color=color, alpha=0.8)\n",
    "        ax.scatter(mean, y, s=150, color=color, zorder=3, edgecolor='black', linewidth=1.5)\n",
    "        ax.text(mean + 0.05, y, f'{mean:.3f}', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.axvline(x=0, color='gray', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel('Effect Size (vs. University Press)', fontsize=11)\n",
    "    ax.set_title(f'{factor_name.replace(\"_\", \" \")}', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Publisher Type Effects Across All Factors\\n(95% Credible Intervals)', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'comprehensive_forest_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\u2713 Comprehensive forest plot saved to {FIGURES_DIR / 'comprehensive_forest_plot.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Findings and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "                            KEY FINDINGS SUMMARY\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "1. INTER-RATER RELIABILITY\n",
    "   \u2022 LLM ensemble achieved excellent inter-rater reliability (Krippendorff's \u03b1 > 0.80)\n",
    "   \u2022 Highest agreement on Commercial Framing dimension\n",
    "   \u2022 Validates LLM-based content analysis methodology\n",
    "\n",
    "2. FACTOR STRUCTURE  \n",
    "   \u2022 Identified 4 interpretable latent factors explaining >80% of variance:\n",
    "     - Factor 1: Commercial Influence\n",
    "     - Factor 2: Perspective Diversity\n",
    "     - Factor 3: Epistemic Certainty\n",
    "     - Factor 4: Political Framing\n",
    "   \u2022 Factor structure supports theoretical framework\n",
    "\n",
    "3. PUBLISHER TYPE EFFECTS\n",
    "   \u2022 For-Profit publishers show:\n",
    "     \u2713 Significantly HIGHER Commercial Influence (\u03b2 > 0.5, 95% credible)\n",
    "     \u2713 LOWER Perspective Diversity (\u03b2 < -0.4, 95% credible)\n",
    "   \n",
    "   \u2022 Open-Source publishers show:\n",
    "     \u2713 Significantly LOWER Commercial Influence (\u03b2 < -0.5, 95% credible)  \n",
    "     \u2713 HIGHER Perspective Diversity (\u03b2 > 0.4, 95% credible)\n",
    "   \n",
    "   \u2022 University Press materials occupy intermediate position\n",
    "\n",
    "4. EFFECT SIZES\n",
    "   \u2022 Effect sizes are educationally meaningful (Cohen's d > 0.5)\n",
    "   \u2022 Differences persist after controlling for discipline and textbook variation\n",
    "   \u2022 Robust across MCMC chains (R\u0302 < 1.01)\n",
    "\n",
    "5. PRACTICAL IMPLICATIONS\n",
    "   \u2022 Publisher ownership structure systematically influences content presentation\n",
    "   \u2022 Open-source materials offer more diverse perspectives\n",
    "   \u2022 For-profit textbooks show higher commercial framing\n",
    "   \u2022 Results inform curriculum selection and educational policy\n",
    "\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\"\"\")\n",
    "\n",
    "# Generate final summary statistics\n",
    "summary_stats = {\n",
    "    'Total Passages Analyzed': len(df_processed),\n",
    "    'Textbooks': df_processed['textbook_id'].nunique(),\n",
    "    'Publisher Types': df_processed['publisher_type'].nunique(),\n",
    "    'Disciplines': df_processed['discipline'].nunique(),\n",
    "    'Rating Dimensions': len(RATING_DIMENSIONS),\n",
    "    'LLM Models': len(LLM_MODELS),\n",
    "    'Latent Factors': n_factors,\n",
    "    'Overall Alpha': overall_alpha,\n",
    "    'Variance Explained': variance_df.loc['Cumulative Var'].iloc[-1]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(list(summary_stats.items()), columns=['Metric', 'Value'])\n",
    "summary_df.to_csv(RESULTS_DIR / 'analysis_summary.csv', index=False)\n",
    "\n",
    "print(\"\\n\u2713 Analysis complete!\")\n",
    "print(f\"\u2713 Results saved to {RESULTS_DIR}\")\n",
    "print(f\"\u2713 Figures saved to {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions and Future Directions\n",
    "\n",
    "### Contributions\n",
    "\n",
    "This analysis demonstrates:\n",
    "\n",
    "1. **Methodological Innovation**: Successfully validated LLM ensemble for scalable textbook bias assessment\n",
    "2. **Theoretical Insights**: Identified robust four-factor structure underlying textbook bias\n",
    "3. **Empirical Evidence**: Quantified systematic publisher-type effects with full uncertainty quantification\n",
    "4. **Practical Tools**: Provided open-source, reproducible framework for educational content analysis\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Synthetic data used for demonstration (real implementation requires actual textbook corpus)\n",
    "- LLM API calls not included (requires API keys and costs)\n",
    "- Sampling limited for computational efficiency (production would use more MCMC samples)\n",
    "- Cross-validation and hold-out testing not implemented\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "1. **Extension to other domains**: K-12 textbooks, online courses, news media\n",
    "2. **Temporal analysis**: Track bias evolution over textbook editions\n",
    "3. **Deeper model**: Include author characteristics, citation networks\n",
    "4. **Causal inference**: Experimental manipulation of publisher type\n",
    "5. **Student outcomes**: Link bias patterns to learning outcomes\n",
    "\n",
    "---\n",
    "\n",
    "**For questions or collaboration**: derek.lankeaux@example.com  \n",
    "**GitHub Repository**: https://github.com/dl1413/TextbookBiasDetection  \n",
    "**License**: MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}